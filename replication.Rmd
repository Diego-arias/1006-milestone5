---
title: "replication"
author: "Diego Arias"
date: "4/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lme4)
library(stargazer)
```

```{r}
### DIEGO: If I am completely honest I do not understand much of what is going on in this code (mostly because we have not touched on generalized linear mixed-effects model)
### DIEGO: Neverthelles, this paper seems to be very interesting, as it uses audio recordings from courts to see if judges implicitly reveal their verdict leaning during oral arguments, even before arguments and deliberations are done

###DIEGO: Ultimately, the experiement shows that the higher emotional arousal or excitement directed at an attorney compared to his or her opponent, the less likely that attorney is to win the Justiceâ€™s vote


### DIEGO: this code is reading in the justice table tab that is a seperate file in the zip
#load justice_results
sc<-read.table("justice_results.tab",header=TRUE,as.is=TRUE,sep="\t")


### DIEGO: this is fitting a generalized linear mixed-effects model using the justices' name to predict the petitioner vote
### DIEGO: the pre_mod is takoing this model and using it to make predictions. I am not quite sure what it really does though 
#intercept only (Table 1, Model 1)

mod0<-glmer(petitioner_vote~1+(1|justiceName),data=sc,family=binomial)
pred_mod0<-sum(diag(table(ifelse(predict(mod0,type="response")>.50,1,0),sc[names(residuals(mod0)),"petitioner_vote"])))/length(residuals(mod0))

#pitch only (Table 1, Model 2)
### DIEGO: this will be fitting a different generalized linear mixed-effects model - with this one using the pitch difference of the judges' voice ot predict the petitioner vote
### DIEGO: pitch difference is measured by subtracting the vocal pitch in questions directed toward petitioners from the vocal pitch in questions directed toward respondents
mod1<-glmer(petitioner_vote~pitch_diff+(1|justiceName),data=sc,family=binomial)
pred_mod1<-sum(diag(table(ifelse(predict(mod1,type="response")>.50,1,0),sc[names(residuals(mod1)),"petitioner_vote"])))/length(residuals(mod1))

#dal model (Table 1, Model 3)

### DIEGO: This code is creating new columns using data from other ones (these new columns code for negative and postive word frequency from the petitioner and respondent)
sc$petitioner_pos_words<-sc$petitioner_dal_pos
sc$petitioner_neg_words<-sc$petitioner_dal_neg
sc$respondent_pos_words<-sc$respondent_dal_pos
sc$respondent_neg_words<-sc$respondent_dal_neg


### DIEGO: This is another glme model. In this case, it seems like there are multiple predictors being used. 
mod2<-glmer(petitioner_vote~pitch_diff+I((petitioner_neg_words/petitioner_wc)-(respondent_neg_words/respondent_wc))+I((petitioner_pos_words/petitioner_wc)-(respondent_pos_words/respondent_wc))+I(petitioner_count-respondent_count)+lagged_ideology+conservative_lc+I(lagged_ideology*conservative_lc)+sgpetac+sgrespac+petac+respac+petNumStat+respNumStat+(1|justiceName),data=sc,family=binomial,nAGQ=2)
pred_mod2<-sum(diag(table(ifelse(predict(mod2,type="response")>.50,1,0),sc[names(residuals(mod2)),"petitioner_vote"])))/length(residuals(mod2))
#the model does not converge unless the number of points per axis for evaluating the adaptive Gauss-Hermite approximation to the log-likelihood is increased from 0. Coefficients and prediction rate are essentiall the same regardless of nAGQ used. More specifically, max coefficient change is around 10^-04

#harvard model (Table 1, Model 4)
sc$petitioner_pos_words<-sc$petitioner_harvard_pos
sc$petitioner_neg_words<-sc$petitioner_harvard_neg
sc$respondent_pos_words<-sc$respondent_harvard_pos
sc$respondent_neg_words<-sc$respondent_harvard_neg

mod3<-glmer(petitioner_vote~pitch_diff+I((petitioner_neg_words/petitioner_wc)-(respondent_neg_words/respondent_wc))+I((petitioner_pos_words/petitioner_wc)-(respondent_pos_words/respondent_wc))+I(petitioner_count-respondent_count)+lagged_ideology+conservative_lc+I(lagged_ideology*conservative_lc)+sgpetac+sgrespac+petac+respac+petNumStat+respNumStat+(1|justiceName),data=sc,family=binomial)
pred_mod3<-sum(diag(table(ifelse(predict(mod3,type="response")>.50,1,0),sc[names(residuals(mod3)),"petitioner_vote"])))/length(residuals(mod3))

#liwc model (Table 1, Model 5)
sc$petitioner_pos_words<-sc$petitioner_liwc_pos
sc$petitioner_neg_words<-sc$petitioner_liwc_neg
sc$respondent_pos_words<-sc$respondent_liwc_pos
sc$respondent_neg_words<-sc$respondent_liwc_neg

mod4<-glmer(petitioner_vote~pitch_diff+I((petitioner_neg_words/petitioner_wc)-(respondent_neg_words/respondent_wc))+I((petitioner_pos_words/petitioner_wc)-(respondent_pos_words/respondent_wc))+I(petitioner_count-respondent_count)+lagged_ideology+conservative_lc+I(lagged_ideology*conservative_lc)+sgpetac+sgrespac+petac+respac+petNumStat+respNumStat+(1|justiceName),data=sc,family=binomial,nAGQ=2)
pred_mod4<-sum(diag(table(ifelse(predict(mod4,type="response")>.50,1,0),sc[names(residuals(mod4)),"petitioner_vote"])))/length(residuals(mod4))
#the model does not converge unless the number of points per axis for evaluating the adaptive Gauss-Hermite approximation to the log-likelihood is increased from 0. Coefficients and prediction rate are essentiall the same regardless of nAGQ used. More specifically, max coefficent change is around 10^-04

stargazer(mod0,mod1,mod2,mod3,mod4,type='html',out='table_1.html',intercept.bottom = FALSE, intercept.top = TRUE, omit.stat = c('bic'), dep.var.labels.include = FALSE, dep.var.caption = "", column.labels = c('intercept only','no controls','dal','harvard','liwc'))
```

